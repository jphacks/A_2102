# -*- coding: utf-8 -*-
"""Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TEVr9GmVm3odGl7ed_TaA8NSDQw_8xNF
"""

import requests
import re
from bs4 import BeautifulSoup

def scrape(search_word, pages_num):

  print(f'【検索ワード】{search_word}')

  # Googleから検索結果ページを取得する
  url = f'https://www.google.co.jp/search?hl=ja&num={pages_num}&q={search_word}'
  request = requests.get(url)

  # Googleのページ解析を行う
  soup = BeautifulSoup(request.text, "html.parser")
  search_site_list = soup.select('div.kCrYT > a')

  #正規表現：日本語
  nihongo = re.compile('[ぁ-んァ-ン一-龥ー、。「」（）！？・…”’【】『』\-0-9^kcal$]+')
  nandemo = re.compile('.+')

  # ページ解析と結果の出力
  for rank, site in zip(range(1, pages_num), search_site_list):
      site_url_raw = site['href'].replace('/url?q=', '')
      site_url = site_url_raw[:site_url_raw.find("&sa")]
      print(site_url)
      html = requests.get(site_url).content
      site_soup = BeautifulSoup(html,'html.parser')

      body = site_soup.find('main-content')
      if body is None:
        body = site_soup.find('main')
        #print('main')
      if body is None:
        body = site_soup.find('body')
        #print('body')

      

      the_contents_of_body_without_body_tags = body.findChildren(recursive=False)
      contents = []
      for i in range(len(the_contents_of_body_without_body_tags)):
        contents.extend(re.findall(nandemo, str(the_contents_of_body_without_body_tags[i])))

      #1
      #print('#1')
      #print(contents)

      #for i in range(len(the_contents_of_body_without_body_tags)):
      #  contents.extend(str(the_contents_of_body_without_body_tags[i]))

      pattern = re.compile(r'、|。')
      contents = [i for i in contents if pattern.search(i)]

      #print('#2')
      #print(contents)

      the_contents_of_body_without_body_tags.clear()
      for i in range(len(contents)):
        the_contents_of_body_without_body_tags.append(contents[i])

      contents.clear()

      for i in range(len(the_contents_of_body_without_body_tags)):
        contents.extend(re.findall(nihongo, str(the_contents_of_body_without_body_tags[i])))

      #print('#3')
      #print(contents)

      contents = [i for i in contents if len(i) > 4]

      #print('#4')
      #print(contents)

      #print('#5')
      the_contents_of_body_without_body_tags.clear()
      the_contents_of_body_without_body_tags = "".join(contents)
      #for i in range(len(contents)):
      #  the_contents_of_body_without_body_tags.extend(contents[i])

      contents.clear()
      contents = the_contents_of_body_without_body_tags.split('。')

      print(contents)
      print('#文の数：' + str(len(contents)))

scrape("JPHACKS", 10)